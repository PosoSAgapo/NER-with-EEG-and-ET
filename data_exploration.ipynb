{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T11:19:28.923171Z",
     "start_time": "2019-09-15T11:19:25.726796Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as io\n",
    "import gzip\n",
    "import math\n",
    "import os\n",
    "import re\n",
    "import scipy\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: still need to understand why those lines in Maria's code are necessary\n",
    "\n",
    "zurich['BNCfreq'] = zurich.WORDstrip.map(lambda x: unigrdict.get(str(x).lower()))\n",
    "zurich.BNCfreq = zurich.BNCfreq.fillna(zurich.BNCfreq.min())\n",
    "zurich.BNCfreq = zurich.BNCfreq/100 #because 100 million word - to get freq per million\n",
    "zurich.BNCfreq = np.log(zurich.BNCfreq)\n",
    "\n",
    "zurich['BNCfreqinv']= -zurich.BNCfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T10:41:28.480227Z",
     "start_time": "2019-09-15T10:41:28.440122Z"
    },
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def mk_dataframe(task:str, subject:int, level:str, scaling:str):\n",
    "    \"\"\"\n",
    "        Args: Task number (\"task1\", \"task2\", \"task3\" , test subject(0-11).\n",
    "        Return: DataFrame on word level.\n",
    "    \"\"\"\n",
    "    bnc_freq = get_bncfreq()\n",
    "    files = get_matfiles(task)\n",
    "    data = io.loadmat(files[subject], squeeze_me=True, struct_as_record=False)['sentenceData']\n",
    "    \n",
    "    if level == 'sentence':\n",
    "        fields = ['SentLen', 'omissionRate', 'nFixations', 'meanPupilSize', 'GD', 'TRT', 'FFD', 'SFD', \n",
    "                  'GPT', 'BNCFreq']\n",
    "        features = np.zeros((len(data), len(fields)))\n",
    "        \n",
    "    elif level == 'word':\n",
    "        n_words = sum([len(sent.word) for sent in data])\n",
    "        fields = ['Sent_ID', 'Word_ID', 'Word', 'nFixations', 'meanPupilSize', \n",
    "                  'GD', 'TRT', 'FFD', 'SFD', 'GPT', 'WordLen', 'BNCFreq']\n",
    "        df = pd.DataFrame(index=range(n_words), columns=[fields])\n",
    "        k = 0\n",
    "    else:\n",
    "        raise Exception('Data can only be processed on sentence or word level')\n",
    "        \n",
    "    for i, sent in enumerate(data):\n",
    "        print('Iter:', i)\n",
    "        print()\n",
    "        print([word.content for word in sent.word])\n",
    "        print()\n",
    "        for j, word in enumerate(sent.word):\n",
    "            token = re.sub('[^\\w\\s]', '', word.content)\n",
    "            token = token.lower() if j == 0 else token\n",
    "            if level == 'sentence':\n",
    "                features[i,2:-1] += [getattr(word, field) if hasattr(word, field)\\\n",
    "                                    and not isinstance(getattr(word, field), np.ndarray) else\\\n",
    "                                    0 for field in fields[2:-1]]\n",
    "                #TODO: figure out whether divsion by 100 leads to -inf values after log computation \n",
    "                features[i,-1] += np.log(bnc_freq[token]/100) if bnc_freq[token]/100 != 0 else 0\n",
    "            else:\n",
    "                df.iloc[k, 0] = str(i) + '_NR' if task=='task1' or task=='task2' else str(i) + '_TSR'\n",
    "                df.iloc[k, 1] = j\n",
    "                df.iloc[k, 2] = token\n",
    "                df.iloc[k, 3:-2] = [getattr(word, field) if hasattr(word, field)\\\n",
    "                                    and not isinstance(getattr(word, field), np.ndarray) else\\\n",
    "                                    0 for field in fields[3:-2]]\n",
    "                df.iloc[k, -2] = len(token)\n",
    "                #TODO: figure out whether divsion by 100 leads to -inf values after log computation\n",
    "                df.iloc[k,-1] = np.log(bnc_freq[token]/100) if bnc_freq[token]/100 != 0 else 0\n",
    "                k += 1\n",
    "                \n",
    "        if level == 'sentence':\n",
    "            features[i, 0] = len(sent.word)\n",
    "            features[i, 1] = sent.omissionRate\n",
    "            features[i, 2:] /= len(sent.word)\n",
    "    \n",
    "    #handle -inf, inf and NaN values\n",
    "    #if level == 'sentence': \n",
    "    #    features = inf_check(features)\n",
    "    if level == 'word':\n",
    "        df.iloc[:,:-1].fillna(0, inplace=True)\n",
    "        df.iloc[:,-1].fillna(getattr(df,fields[-1]).values.min(), inplace=True)  \n",
    "        df.replace([np.inf, -np.inf], np.nan).dropna(axis=0, inplace=True)\n",
    "\n",
    "    #normalize data featurewise\n",
    "    if scaling == 'min-max':\n",
    "        if level == 'sentence':\n",
    "            features = np.array([(feat - min(feat))/(max(feat) - min(feat)) for feat in features.T])\n",
    "            df = pd.DataFrame(data=features.T, index=range(features.shape[1]), columns=[fields])\n",
    "        elif level == 'word':\n",
    "            df.iloc[:, 3:] = [(getattr(df,field).values - getattr(df,field).values.min())/\\\n",
    "                              (getattr(df,field).values.max() - getattr(df,field).values.min())\\\n",
    "                              for field in fields[3:]]\n",
    "    elif scaling == 'standard':\n",
    "        if level == 'sentence':\n",
    "            features = np.array([(feat - np.mean(feat))/np.std(feat) for feat in features.T])\n",
    "            df = pd.DataFrame(data=features.T, index=range(features.shape[1]), columns=[fields])\n",
    "        elif level == 'word':\n",
    "            df.iloc[:, 3:] = [(getattr(df,field).values - getattr(df,field).values.mean())/\\\n",
    "                              getattr(df,field).values.std() for field in fields[3:]]\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T10:41:39.838160Z",
     "start_time": "2019-09-15T10:41:29.210731Z"
    }
   },
   "outputs": [],
   "source": [
    "#sbj3 = mk_dataframe('task1', 2, level='sentence', scaling='min-max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-15T11:28:36.265005Z",
     "start_time": "2019-09-15T11:28:36.260993Z"
    }
   },
   "outputs": [],
   "source": [
    "# instantiate data transformer object for task 1 (NR) on sentence level with min-max feature scaling\n",
    "datatransform = DataTransformer('task1', level='sentence', scaling='min-max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2019-09-15T11:28:39.307Z"
    }
   },
   "outputs": [],
   "source": [
    "# get data for all subjects according to settings in cell above\n",
    "# NOTE: data for each sbj is stored in a Pandas DataFrame\n",
    "sbjs = [datatransform(0), datatransform(1), datatransform(2), \n",
    "        datatransform(3), datatransform(4), datatransform(5), \n",
    "        datatransform(6), datatransform(7),datatransform(8),\n",
    "       datatransform(9), datatransform(10), datatransform(11)]"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
