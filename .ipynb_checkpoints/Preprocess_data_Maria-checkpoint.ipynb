{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-13T08:48:46.352352Z",
     "start_time": "2019-09-13T08:48:46.346336Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import codecs\n",
    "from scipy.io import loadmat\n",
    "import glob\n",
    "import csv\n",
    "import html\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib\n",
    "%matplotlib inline  \n",
    "import random \n",
    "random.seed(99)\n",
    "\n",
    "pd.options.display.max_columns = 1000\n",
    "pd.options.display.max_rows = 1000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To reproduce load model and predict on wh_both dev set:\n",
    "\n",
    "source ~/.bash rc\n",
    "tensorflow\n",
    "cd ~/Repos/attention_attention/mltagger/exp_scripts/\n",
    "sudo ./exp_load.sh  bl /Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-fce-bl-32-1.0-0.4-1.0-66-0.25 \n",
    "./exp_load.sh Mean_fix_dur /Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-fce-Mean_fix_dur-32-0.1-0.2-1.0-66-1.0\n",
    "/Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-semeval_neg_sent-bl-32-1.0-0.4-1.0-66-0.25\n",
    "/Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-semeval_neg_sent-Mean_fix_dur-32-1.0-0.8-1.0-66-decreasing\n",
    "/Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-semeval_pos_sent-bl-32-0.1-0.4-1.0-66-decreasing\n",
    "./exp_load.sh Mean_fix_dur /Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-semeval_pos_sent-Mean_fix_dur-32-0.1-0.4-1.0-66-0.125\n",
    "\n",
    "./exp_load.sh bl /Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-wh_both-bl-32-0.1-0.2-1.0-66-0.125\n",
    "./exp_load.sh Mean_fix_dur /Users/Maria/Repos/attention_attention/mltagger/exp/testoutput/test-wh_both-Mean_fix_dur-32-0.1-0.6-1.0-66-decreasing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testoutput/test-fce-bl-32-1.0-0.4-1.0-66-0.25/\n",
      "testoutput/test-fce-Mean_fix_dur-32-0.1-0.2-1.0-66-1.0/\n",
      "testoutput/test-semeval_neg_sent-bl-32-1.0-0.4-1.0-66-0.25/\n",
      "testoutput/test-semeval_neg_sent-Mean_fix_dur-32-1.0-0.8-1.0-66-decreasing/\n",
      "testoutput/test-wh_both-bl-32-0.1-0.2-1.0-66-0.125/\n",
      "testoutput/test-wh_both-Mean_fix_dur-32-0.1-0.6-1.0-66-decreasing/\n"
     ]
    }
   ],
   "source": [
    "old_foldernames = [\"aws5/wh_both_bl-32-0.1-1.0-1.0-66-decreasing/\",\n",
    "                \"aws5/wh_both-Mean_fix_dur-32-0.1-1.0-1.0-66-decreasing/\",\n",
    "               \"aws3/semeval_neg_sent_bl-32-1.0-0.4-1.0-66-0.25/\",\n",
    "               \"aws3/semeval_neg_sent-Mean_fix_dur-32-1.0-0.6-1.0-66-0.5/\",\n",
    "               \"aws2/fce_bl-32-1.0-0.6-1.0-66-0.25/\",\n",
    "               \"aws2/fce-Mean_fix_dur-32-1.0-1.0-1.0-66-0.25/\"\n",
    "              ]\n",
    "\n",
    "foldernames = [\"testoutput/test-fce-bl-32-1.0-0.4-1.0-66-0.25/\",\n",
    "               \"testoutput/test-fce-Mean_fix_dur-32-0.1-0.2-1.0-66-1.0/\",\n",
    "               \"testoutput/test-semeval_neg_sent-bl-32-1.0-0.4-1.0-66-0.25/\",\n",
    "               \"testoutput/test-semeval_neg_sent-Mean_fix_dur-32-1.0-0.8-1.0-66-decreasing/\",\n",
    "               #\"testoutput/test-semeval_pos_sent-bl-32-0.1-0.4-1.0-66-decreasing/\",\n",
    "               #\"testoutput/test-semeval_pos_sent-Mean_fix_dur-32-0.1-0.4-1.0-66-0.125/\",\n",
    "               \"testoutput/test-wh_both-bl-32-0.1-0.2-1.0-66-0.125/\",\n",
    "               \"testoutput/test-wh_both-Mean_fix_dur-32-0.1-0.6-1.0-66-decreasing/\"\n",
    "]\n",
    "domain = \"wh_both\"\n",
    "alldoms = pd.DataFrame()\n",
    "for folder in foldernames:\n",
    "    print(folder)\n",
    "    dom = '-'.join([folder[11:].split('-')[2],folder[11:].split('-')[1]])\n",
    "    allsents = pd.DataFrame()\n",
    "    folderpath = \"mltagger/exp/\"+folder\n",
    "    outputfile = folderpath+\"modelload_predictions_dv_\"+domain+\"/output.txt\"\n",
    "    filelist = [li.split('\\n') for li in open(outputfile).read().split('\\n\\n')]\n",
    "    wordsplit = [[w.split('\\t') for w in sent] for sent in filelist]\n",
    "    \n",
    "    for sent in wordsplit[:50]:\n",
    "        if sent[0][0] == '1': #if gold label is 1:\n",
    "            #print(sent[0])\n",
    "            onesent = pd.DataFrame.from_records(sent[1:], columns=['Word', 'Label', 'Pred_gaze', 'Attention']) #not keeping the label rows\n",
    "            onesent = onesent[['Word', 'Attention']]\n",
    "            onesent.Attention = onesent.Attention.astype(float)\n",
    "            mean = onesent.Attention.mean()\n",
    "            onesent.loc[onesent.Attention > mean, 'Word'] = onesent[onesent.Attention > mean][\"Word\"].apply(lambda x: \"\\textbf{\"+str(x)+\"}\")\n",
    "            allsents = pd.concat([allsents, onesent], axis=0)\n",
    "    allsents.columns = [dom , 'attention']\n",
    "    allsents.reset_index(drop=True, inplace=True)\n",
    "\n",
    "    alldoms = pd.concat([alldoms, allsents[[dom]]], axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alldoms[7000:7500]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bl-fce</th>\n",
       "      <th>Mean_fix_dur-fce</th>\n",
       "      <th>bl-semeval_neg_sent</th>\n",
       "      <th>Mean_fix_dur-semeval_neg_sent</th>\n",
       "      <th>bl-wh_both</th>\n",
       "      <th>Mean_fix_dur-wh_both</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>180</th>\n",
       "      <td>@CharlesClassiqk:</td>\n",
       "      <td>\\textbf{@CharlesClassiqk:}</td>\n",
       "      <td>\\textbf{@CharlesClassiqk:}</td>\n",
       "      <td>\\textbf{@CharlesClassiqk:}</td>\n",
       "      <td>@CharlesClassiqk:</td>\n",
       "      <td>@CharlesClassiqk:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>181</th>\n",
       "      <td>sorry</td>\n",
       "      <td>\\textbf{sorry}</td>\n",
       "      <td>\\textbf{sorry}</td>\n",
       "      <td>\\textbf{sorry}</td>\n",
       "      <td>sorry</td>\n",
       "      <td>sorry</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>182</th>\n",
       "      <td>\\textbf{I'm}</td>\n",
       "      <td>I'm</td>\n",
       "      <td>\\textbf{I'm}</td>\n",
       "      <td>\\textbf{I'm}</td>\n",
       "      <td>I'm</td>\n",
       "      <td>I'm</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>183</th>\n",
       "      <td>\\textbf{not}</td>\n",
       "      <td>not</td>\n",
       "      <td>\\textbf{not}</td>\n",
       "      <td>\\textbf{not}</td>\n",
       "      <td>not</td>\n",
       "      <td>not</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184</th>\n",
       "      <td>\\textbf{sexist}</td>\n",
       "      <td>\\textbf{sexist}</td>\n",
       "      <td>\\textbf{sexist}</td>\n",
       "      <td>\\textbf{sexist}</td>\n",
       "      <td>sexist</td>\n",
       "      <td>sexist</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185</th>\n",
       "      <td>BUT</td>\n",
       "      <td>BUT</td>\n",
       "      <td>BUT</td>\n",
       "      <td>BUT</td>\n",
       "      <td>BUT</td>\n",
       "      <td>\\textbf{BUT}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>there</td>\n",
       "      <td>\\textbf{there}</td>\n",
       "      <td>there</td>\n",
       "      <td>there</td>\n",
       "      <td>there</td>\n",
       "      <td>\\textbf{there}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187</th>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "      <td>is</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>188</th>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "      <td>a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>189</th>\n",
       "      <td>double</td>\n",
       "      <td>\\textbf{double}</td>\n",
       "      <td>double</td>\n",
       "      <td>\\textbf{double}</td>\n",
       "      <td>double</td>\n",
       "      <td>\\textbf{double}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>standards</td>\n",
       "      <td>\\textbf{standards}</td>\n",
       "      <td>standards</td>\n",
       "      <td>\\textbf{standards}</td>\n",
       "      <td>\\textbf{standards}</td>\n",
       "      <td>\\textbf{standards}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>191</th>\n",
       "      <td>there's</td>\n",
       "      <td>\\textbf{there's}</td>\n",
       "      <td>there's</td>\n",
       "      <td>there's</td>\n",
       "      <td>\\textbf{there's}</td>\n",
       "      <td>\\textbf{there's}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>192</th>\n",
       "      <td>certain</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>certain</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>193</th>\n",
       "      <td>rules</td>\n",
       "      <td>rules</td>\n",
       "      <td>rules</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>194</th>\n",
       "      <td>\\textbf{for}</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>\\textbf{for}</td>\n",
       "      <td>\\textbf{for}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>195</th>\n",
       "      <td>\\textbf{dudes}</td>\n",
       "      <td>dudes</td>\n",
       "      <td>dudes</td>\n",
       "      <td>dudes</td>\n",
       "      <td>\\textbf{dudes}</td>\n",
       "      <td>\\textbf{dudes}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>196</th>\n",
       "      <td>\\textbf{and}</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>and</td>\n",
       "      <td>\\textbf{and}</td>\n",
       "      <td>and</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>197</th>\n",
       "      <td>\\textbf{there's}</td>\n",
       "      <td>\\textbf{there's}</td>\n",
       "      <td>there's</td>\n",
       "      <td>there's</td>\n",
       "      <td>\\textbf{there's}</td>\n",
       "      <td>\\textbf{there's}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>198</th>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>certain</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "      <td>\\textbf{certain}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>199</th>\n",
       "      <td>\\textbf{rules}</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "      <td>rules</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "      <td>\\textbf{rules}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>200</th>\n",
       "      <td>\\textbf{for}</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>for</td>\n",
       "      <td>\\textbf{for}</td>\n",
       "      <td>for</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>201</th>\n",
       "      <td>\\textbf{fem…}</td>\n",
       "      <td>\\textbf{fem…}</td>\n",
       "      <td>fem…</td>\n",
       "      <td>fem…</td>\n",
       "      <td>\\textbf{fem…}</td>\n",
       "      <td>fem…</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                bl-fce            Mean_fix_dur-fce  \\\n",
       "180  @CharlesClassiqk:  \\textbf{@CharlesClassiqk:}   \n",
       "181              sorry              \\textbf{sorry}   \n",
       "182       \\textbf{I'm}                         I'm   \n",
       "183       \\textbf{not}                         not   \n",
       "184    \\textbf{sexist}             \\textbf{sexist}   \n",
       "185                BUT                         BUT   \n",
       "186              there              \\textbf{there}   \n",
       "187                 is                          is   \n",
       "188                  a                           a   \n",
       "189             double             \\textbf{double}   \n",
       "190          standards          \\textbf{standards}   \n",
       "191            there's            \\textbf{there's}   \n",
       "192            certain            \\textbf{certain}   \n",
       "193              rules                       rules   \n",
       "194       \\textbf{for}                         for   \n",
       "195     \\textbf{dudes}                       dudes   \n",
       "196       \\textbf{and}                         and   \n",
       "197   \\textbf{there's}            \\textbf{there's}   \n",
       "198   \\textbf{certain}            \\textbf{certain}   \n",
       "199     \\textbf{rules}              \\textbf{rules}   \n",
       "200       \\textbf{for}                         for   \n",
       "201      \\textbf{fem…}               \\textbf{fem…}   \n",
       "\n",
       "            bl-semeval_neg_sent Mean_fix_dur-semeval_neg_sent  \\\n",
       "180  \\textbf{@CharlesClassiqk:}    \\textbf{@CharlesClassiqk:}   \n",
       "181              \\textbf{sorry}                \\textbf{sorry}   \n",
       "182                \\textbf{I'm}                  \\textbf{I'm}   \n",
       "183                \\textbf{not}                  \\textbf{not}   \n",
       "184             \\textbf{sexist}               \\textbf{sexist}   \n",
       "185                         BUT                           BUT   \n",
       "186                       there                         there   \n",
       "187                          is                            is   \n",
       "188                           a                             a   \n",
       "189                      double               \\textbf{double}   \n",
       "190                   standards            \\textbf{standards}   \n",
       "191                     there's                       there's   \n",
       "192                     certain              \\textbf{certain}   \n",
       "193                       rules                \\textbf{rules}   \n",
       "194                         for                           for   \n",
       "195                       dudes                         dudes   \n",
       "196                         and                           and   \n",
       "197                     there's                       there's   \n",
       "198                     certain              \\textbf{certain}   \n",
       "199                       rules                \\textbf{rules}   \n",
       "200                         for                           for   \n",
       "201                        fem…                          fem…   \n",
       "\n",
       "             bl-wh_both Mean_fix_dur-wh_both  \n",
       "180   @CharlesClassiqk:    @CharlesClassiqk:  \n",
       "181               sorry                sorry  \n",
       "182                 I'm                  I'm  \n",
       "183                 not                  not  \n",
       "184              sexist               sexist  \n",
       "185                 BUT         \\textbf{BUT}  \n",
       "186               there       \\textbf{there}  \n",
       "187                  is                   is  \n",
       "188                   a                    a  \n",
       "189              double      \\textbf{double}  \n",
       "190  \\textbf{standards}   \\textbf{standards}  \n",
       "191    \\textbf{there's}     \\textbf{there's}  \n",
       "192    \\textbf{certain}     \\textbf{certain}  \n",
       "193      \\textbf{rules}       \\textbf{rules}  \n",
       "194        \\textbf{for}         \\textbf{for}  \n",
       "195      \\textbf{dudes}       \\textbf{dudes}  \n",
       "196        \\textbf{and}                  and  \n",
       "197    \\textbf{there's}     \\textbf{there's}  \n",
       "198    \\textbf{certain}     \\textbf{certain}  \n",
       "199      \\textbf{rules}       \\textbf{rules}  \n",
       "200        \\textbf{for}                  for  \n",
       "201       \\textbf{fem…}                 fem…  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alldoms[180:202]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllll}\n",
      "\\toprule\n",
      "            bl-fce &            Mean_fix_dur-fce &         bl-semeval_neg_sent & Mean_fix_dur-semeval_neg_sent &          bl-wh_both & Mean_fix_dur-wh_both \\\\\n",
      "\\midrule\n",
      " @CharlesClassiqk: &  \\textbf{@CharlesClassiqk:} &  \\textbf{@CharlesClassiqk:} &    \\textbf{@CharlesClassiqk:} &   @CharlesClassiqk: &    @CharlesClassiqk: \\\\\n",
      "             sorry &              \\textbf{sorry} &              \\textbf{sorry} &                \\textbf{sorry} &               sorry &                sorry \\\\\n",
      "      \\textbf{I'm} &                         I'm &                \\textbf{I'm} &                  \\textbf{I'm} &                 I'm &                  I'm \\\\\n",
      "      \\textbf{not} &                         not &                \\textbf{not} &                  \\textbf{not} &                 not &                  not \\\\\n",
      "   \\textbf{sexist} &             \\textbf{sexist} &             \\textbf{sexist} &               \\textbf{sexist} &              sexist &               sexist \\\\\n",
      "               BUT &                         BUT &                         BUT &                           BUT &                 BUT &         \\textbf{BUT} \\\\\n",
      "             there &              \\textbf{there} &                       there &                         there &               there &       \\textbf{there} \\\\\n",
      "                is &                          is &                          is &                            is &                  is &                   is \\\\\n",
      "                 a &                           a &                           a &                             a &                   a &                    a \\\\\n",
      "            double &             \\textbf{double} &                      double &               \\textbf{double} &              double &      \\textbf{double} \\\\\n",
      "         standards &          \\textbf{standards} &                   standards &            \\textbf{standards} &  \\textbf{standards} &   \\textbf{standards} \\\\\n",
      "           there's &            \\textbf{there's} &                     there's &                       there's &    \\textbf{there's} &     \\textbf{there's} \\\\\n",
      "           certain &            \\textbf{certain} &                     certain &              \\textbf{certain} &    \\textbf{certain} &     \\textbf{certain} \\\\\n",
      "             rules &                       rules &                       rules &                \\textbf{rules} &      \\textbf{rules} &       \\textbf{rules} \\\\\n",
      "      \\textbf{for} &                         for &                         for &                           for &        \\textbf{for} &         \\textbf{for} \\\\\n",
      "    \\textbf{dudes} &                       dudes &                       dudes &                         dudes &      \\textbf{dudes} &       \\textbf{dudes} \\\\\n",
      "      \\textbf{and} &                         and &                         and &                           and &        \\textbf{and} &                  and \\\\\n",
      "  \\textbf{there's} &            \\textbf{there's} &                     there's &                       there's &    \\textbf{there's} &     \\textbf{there's} \\\\\n",
      "  \\textbf{certain} &            \\textbf{certain} &                     certain &              \\textbf{certain} &    \\textbf{certain} &     \\textbf{certain} \\\\\n",
      "    \\textbf{rules} &              \\textbf{rules} &                       rules &                \\textbf{rules} &      \\textbf{rules} &       \\textbf{rules} \\\\\n",
      "      \\textbf{for} &                         for &                         for &                           for &        \\textbf{for} &                  for \\\\\n",
      "     \\textbf{fem…} &               \\textbf{fem…} &                        fem… &                          fem… &       \\textbf{fem…} &                 fem… \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(alldoms[180:202].to_latex(index=False, escape=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Waseem Hovy Hatespeech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 403,
   "metadata": {},
   "outputs": [],
   "source": [
    "def splitdataset(ssents):\n",
    "    nsents = len(ssents)\n",
    "    dv_startidx=int(nsents*0.8)\n",
    "    te_start_idx=np.int(nsents*0.9)\n",
    "\n",
    "    tr = ssents[:dv_startidx]\n",
    "    dv = ssents[dv_startidx:te_start_idx]\n",
    "    te = ssents[te_start_idx:]\n",
    "    return tr, dv, te\n",
    "\n",
    "def printtofile(ssents, outfilename):\n",
    "\n",
    "    tr, dv, te = splitdataset(ssents)\n",
    "    print(len(tr))\n",
    "    print(len(te))\n",
    "    print(len(dv))\n",
    "    \n",
    "    allsplit = [tr, dv, te]\n",
    "    splitnames = ['tr', 'dv', 'te']\n",
    "    for i, split in enumerate(allsplit):\n",
    "        splitname = splitnames[i]\n",
    "        outfile=codecs.open(outfilename+'.'+splitname+'.tsv', mode='w', encoding='utf-8')\n",
    "\n",
    "        for sent in split:\n",
    "            if len(sent) == 2:\n",
    "                print(sent[0], file=outfile)\n",
    "                words = sent[1]\n",
    "                words = words.split()\n",
    "                [print(word+'\\t_', file=outfile) for word in words]\n",
    "                print('\\n', file=outfile)\n",
    "        outfile.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14031"
      ]
     },
     "execution_count": 389,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wh = pd.read_json('Data/hatespeech_data/waseem_hovy.json', lines=True, encoding='utf-8')\n",
    "wh = wh[['Annotation', 'text']]\n",
    "len(wh)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6909"
      ]
     },
     "execution_count": 390,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#very skewed distribution\n",
    "w = pd.read_json('Data/hatespeech_data/waseem.json', lines=True, encoding='utf-8')\n",
    "w = w[['Annotation', 'text']]\n",
    "len(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maria/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "#preprocess wh\n",
    "wh.text = wh.text.str.replace('\\n', '')\n",
    "wh.text = wh.text.str.replace('\\t', '')\n",
    "\n",
    "sexism = wh.loc[(wh.Annotation == 'sexism') | (wh.Annotation == 'none')]\n",
    "racism = wh.loc[(wh.Annotation == 'racism') | (wh.Annotation == 'none')]\n",
    "\n",
    "sexism.Annotation.replace('sexism', '+', inplace=True)\n",
    "sexism.Annotation.replace('none', '-', inplace=True)\n",
    "\n",
    "racism.Annotation.replace('racism', '+', inplace=True)\n",
    "racism.Annotation.replace('none', '-', inplace=True)\n",
    "\n",
    "sexism.to_csv('Data/hatespeech_data/wh_sexism.tsv', sep='\\t', encoding='utf-8', index=None, header=None)\n",
    "racism.to_csv('Data/hatespeech_data/wh_racism.tsv', sep='\\t', encoding='utf-8', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Maria/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py:4619: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self._update_inplace(new_data)\n"
     ]
    }
   ],
   "source": [
    "#preprocess w\n",
    "\n",
    "w.text = w.text.str.replace('\\n', '')\n",
    "w.text = w.text.str.replace('\\t', '')\n",
    "\n",
    "sexism = w.loc[(w.Annotation == 'Sexism') | (w.Annotation == 'Neither') | (w.Annotation == 'Both') ]\n",
    "racism = w.loc[(w.Annotation == 'Racism') | (w.Annotation == 'Neither') | (w.Annotation == 'Both')]\n",
    "\n",
    "sexism.Annotation.replace('Sexism', '+', inplace=True)\n",
    "sexism.Annotation.replace('Both', '+', inplace=True)\n",
    "sexism.Annotation.replace('Neither', '-', inplace=True)\n",
    "\n",
    "racism.Annotation.replace('Racism', '+', inplace=True)\n",
    "racism.Annotation.replace('Both', '+', inplace=True)\n",
    "racism.Annotation.replace('Neither', '-', inplace=True)\n",
    "\n",
    "sexism.to_csv('Data/hatespeech_data/w_sexism.tsv', sep='\\t', encoding='utf-8', index=None, header=None)\n",
    "racism.to_csv('Data/hatespeech_data/w_racism.tsv', sep='\\t', encoding='utf-8', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [],
   "source": [
    "#preprocess wh and join sexism and racism in on category\n",
    "\n",
    "both = wh.copy(deep=True)\n",
    "\n",
    "both.Annotation.replace('sexism', '+', inplace=True)\n",
    "both.Annotation.replace('racism', '+', inplace=True)\n",
    "both.Annotation.replace('none', '-', inplace=True)\n",
    "\n",
    "both.to_csv('Data/hatespeech_data/wh_both.tsv', sep='\\t', encoding='utf-8', index=None, header=None)\n",
    "\n",
    "bothw = w.copy(deep=True)\n",
    "\n",
    "bothw.Annotation.replace('Sexism', '+', inplace=True)\n",
    "bothw.Annotation.replace('Racism', '+', inplace=True)\n",
    "bothw.Annotation.replace('Both', '+', inplace=True)\n",
    "bothw.Annotation.replace('Neither', '-', inplace=True)\n",
    "\n",
    "bothw.to_csv('Data/hatespeech_data/w_both.tsv', sep='\\t', encoding='utf-8', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5449\n",
      "682\n",
      "681\n",
      "4799\n",
      "600\n",
      "600\n"
     ]
    }
   ],
   "source": [
    "sexismf = codecs.open('Data/hatespeech_data/w_sexism.tsv', 'r', 'utf-8').read().split('\\n')\n",
    "w_ssents = [li.split('\\t') for li in sexismf]\n",
    "racismf = codecs.open('Data/hatespeech_data/w_racism.tsv', 'r', 'utf-8').read().split('\\n')\n",
    "w_rsents = [li.split('\\t') for li in racismf]\n",
    "\n",
    "printtofile(w_ssents , 'Data/hatespeech_data/w.sexism')\n",
    "printtofile(w_rsents, 'Data/hatespeech_data/w.racism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9646\n",
      "1206\n",
      "1206\n",
      "8588\n",
      "1074\n",
      "1073\n"
     ]
    }
   ],
   "source": [
    "sexismf = codecs.open('Data/hatespeech_data/wh_sexism.tsv', 'r', 'utf-8').read().split('\\n')\n",
    "wh_ssents = [li.split('\\t') for li in sexismf]\n",
    "racismf = codecs.open('Data/hatespeech_data/wh_racism.tsv', 'r', 'utf-8').read().split('\\n')\n",
    "wh_rsents = [li.split('\\t') for li in racismf]\n",
    "\n",
    "printtofile(wh_ssents , 'Data/hatespeech_data/wh.sexism')\n",
    "printtofile(wh_rsents, 'Data/hatespeech_data/wh.racism')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11225\n",
      "1404\n",
      "1403\n",
      "Counter({'-': 8760, '+': 5271, '': 1})\n"
     ]
    }
   ],
   "source": [
    "both = codecs.open('Data/hatespeech_data/wh_both.tsv', 'r', 'utf-8').read().split('\\n')\n",
    "both_sents = [li.split('\\t') for li in both]\n",
    "printtofile(both_sents , 'wh.both')\n",
    "print(Counter([sent[0] for sent in both_sents]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5528\n",
      "691\n",
      "691\n",
      "Counter({'-': 5850, '+': 1059, '': 1})\n"
     ]
    }
   ],
   "source": [
    "wboth = codecs.open('Data/hatespeech_data/w_both.tsv', 'r', 'utf-8').read().split('\\n')\n",
    "wboth_sents = [li.split('\\t') for li in wboth]\n",
    "printtofile(wboth_sents , 'Data/hatespeech_data/w.both')\n",
    "print(Counter([sent[0] for sent in wboth_sents]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dundee and ZuCo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "dundee = pd.read_csv('Data/EN_dun.csv', encoding='utf-8', sep='\\t', index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "#No frequency cols, no word length count.\n",
    "dundeedatacols = ['First_fix_dur', 'First_pass_dur', 'Fix_prob', 'Fixation_prob', 'LINE',\n",
    "       'Mean_fix_dur', 'OBLP', 'OLEN', 'Re-read_prob', 'TXFR',\n",
    "       'Tot_fix_dur', 'Tot_regres_from_dur', 'Tot_regres_to_dur', 'WDLP',\n",
    "       'nFix', 'nLong_regres_from', 'nLong_regres_to',\n",
    "       'nRefix', 'nRefixa_mean', 'nRegres_from', 'nRegres_to',\n",
    "       'nRegressions_to', 'n-1_fix_prob', 'n+1_fix_prob', 'n-2_fix_prob',\n",
    "       'n+2_fix_prob', 'n-1_fix_dur', 'n+1_fix_dur', 'n-2_fix_dur',\n",
    "       'n+2_fix_dur', 'RelWDLP', 'RelOBLP', 'LAUN', 'LAUNabs']\n",
    "\n",
    "nora_datacols = ['nFixations','FFD', 'TRT', 'GD', 'SFD']\n",
    "usecols = ['sent-id', 'word-id', 'word']\n",
    "usecols.extend(nora_datacols)\n",
    "\n",
    "joint_datacols = ['First_fix_dur', 'First_pass_dur', 'Mean_fix_dur','Tot_fix_dur', 'nFix', 'BNCfreq', 'BNCfreqinv','WordLen'  ]\n",
    "word_and_joint = ['Word', 'First_fix_dur', 'First_pass_dur', 'Mean_fix_dur','Tot_fix_dur', 'nFix', 'BNCfreq', 'BNCfreqinv', 'WordLen' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2368"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dundee.groupby(['Itemno', 'SentenceID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "24.746199324324323"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(dundee) / len(dundee.groupby(['Itemno', 'SentenceID']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigrfile = codecs.open('Data/BNC_all.al', 'r',encoding='utf-8').read().split('\\n')\n",
    "unilist = [li.split() for li in unigrfile]\n",
    "unigrdict = {}\n",
    "for li in unilist[1:]:\n",
    "    if len(li) == 4:\n",
    "        try:\n",
    "            unigrdict[li[1]]+=float(li[0])\n",
    "        except KeyError:\n",
    "            unigrdict[li[1]]=float(li[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reading in datafiles from NR and SR, grouping by word and rename columns according to dundee col names\n",
    "#concat to zurich\n",
    "\n",
    "allNR = pd.DataFrame(columns=usecols)\n",
    "\n",
    "for fname in glob.glob('Data/word-level-data/data_NR/*.csv'):\n",
    "    onefile = pd.read_csv(fname, index_col=None, encoding='utf-8')\n",
    "    onefile = onefile[usecols]\n",
    "    onefile.replace('[]', 0, inplace=True)\n",
    "    allNR = pd.concat([allNR, onefile])\n",
    "\n",
    "allNR.loc[:,nora_datacols] = allNR[nora_datacols].astype('float')\n",
    "\n",
    "allNR['Mean_fix_dur'] = allNR.TRT.astype(int) / allNR.nFixations.astype(int) #int division by zero returns nan\n",
    "allNR.Mean_fix_dur.fillna(0, inplace=True) #replace nans with 0\n",
    "\n",
    "allNR[nora_datacols] = allNR[nora_datacols].astype('float')\n",
    "allNRgr = allNR.groupby(['sent-id', 'word-id', 'word'], as_index=False).mean()\n",
    "allNRgr.rename(columns={'word':'Word','FFD':'First_fix_dur', 'TRT':'Tot_fix_dur', 'nFixations':'nFix', 'GD':'First_pass_dur'}, inplace=True)\n",
    "allNRgr['sent-id'] = allNRgr['sent-id'].apply(lambda x: str(x)+'_NR')\n",
    "\n",
    "allSR = pd.DataFrame(columns=usecols)\n",
    "\n",
    "for fname in glob.glob('Data/word-level-data/data_SR/*.csv'):\n",
    "    onefile = pd.read_csv(fname, index_col=None, encoding='utf-8')\n",
    "    onefile = onefile[usecols]\n",
    "    onefile.replace('[]', 0, inplace=True)\n",
    "    allSR = pd.concat([allSR, onefile])\n",
    "    \n",
    "allSR['Mean_fix_dur'] = allSR.TRT.astype(int) / allSR.nFixations.astype(int) #int division by zero returns nan\n",
    "allSR.Mean_fix_dur.fillna(0, inplace=True)\n",
    "\n",
    "allSR.loc[:,nora_datacols] = allSR[nora_datacols].astype('float')\n",
    "\n",
    "allSR['Mean_fix_dur'] = allSR.TRT.divide(allSR.nFixations)\n",
    "allSR.Mean_fix_dur.fillna(0, inplace=True) #replace nans with 0\n",
    "\n",
    "allSRgr = allSR.groupby(['sent-id', 'word-id', 'word'], as_index=False).mean()\n",
    "allSRgr.rename(columns={'word':'Word','FFD':'First_fix_dur', 'TRT':'Tot_fix_dur', 'nFixations':'nFix', 'GD':'First_pass_dur'}, inplace=True)\n",
    "allSRgr['sent-id'] = allSRgr['sent-id'].apply(lambda x: str(x)+'_SR')\n",
    "\n",
    "\n",
    "zurich = pd.concat([allSRgr, allNRgr])\n",
    "#zurich = allSRgr #======= only using Sentimentreading! ==========\n",
    "zurich['WORDstrip'] = zurich['Word'].astype('str')\n",
    "zurich.WORDstrip = zurich.WORDstrip.str.replace('[^\\w\\s]','')\n",
    "zurich['WordLen'] = zurich.WORDstrip.str.len()\n",
    "\n",
    "zurich['BNCfreq'] = zurich.WORDstrip.map(lambda x: unigrdict.get(str(x).lower()))\n",
    "zurich.BNCfreq = zurich.BNCfreq.fillna(zurich.BNCfreq.min())\n",
    "zurich.BNCfreq = zurich.BNCfreq/100 #because 100 million word - to get freq per million\n",
    "zurich.BNCfreq = np.log(zurich.BNCfreq)\n",
    "\n",
    "dundee['BNCfreqcount'] = dundee.Word.map(lambda x: unigrdict.get(str(x).lower()))\n",
    "dundee.BNCfreqcount = dundee.BNCfreqcount.fillna(dundee.BNCfreqcount.min())\n",
    "dundee['BNCfreq'] = dundee.BNCfreqcount/100  \n",
    "dundee.BNCfreq = np.log(dundee.BNCfreq)\n",
    "\n",
    "dundee['BNCfreqinv'] = -dundee.BNCfreq\n",
    "zurich['BNCfreqinv']= -zurich.BNCfreq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lllllllllllll}\n",
      "\\toprule\n",
      "{} &         1240 &     1241 &  1242 & 1243 &      1244 &     1245 & 1246 & 1247 &      1248 &  1249 &  1250 &         1251 \\\\\n",
      "\\midrule\n",
      "Word           &  Responsible &  tourism &  lies &   in &  absolute &  respect &  for &  the &  location &   and &   its &  inhabitants \\\\\n",
      "UniversalPOS   &          ADJ &     NOUN &  VERB &  ADP &       ADJ &     NOUN &  ADP &  DET &      NOUN &  CONJ &  PRON &         NOUN \\\\\n",
      "tot\\_fix\\_dur\\_ms &          319 &      271 &   319 &  169 &       255 &      208 &  162 &  191 &       205 &   108 &   147 &          382 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"len(dundee)\n",
    "dundee['tot_fix_dur_ms'] = np.round(dundee.Tot_fix_dur *1000, 0)\n",
    "\n",
    "print(dundee[['Word', 'UniversalPOS', 'tot_fix_dur_ms']][1240:1252].T.to_latex())\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#No wlen bins and no freq bins\n",
    "def get_bin_stdfrommean(dundee):\n",
    "    for col in joint_datacols:\n",
    "        dundee[col+'_mean'] = dundee[col].mean()\n",
    "        dundee[col+'_std'] = dundee[col].std()\n",
    "        dundee[col+'_stdfrommean'] = (dundee[col] - dundee[col+'_mean']) / dundee[col+'_std']\n",
    "        dundee[col+'_stdfrommeanbin'] = 0\n",
    "        dundee.loc[dundee[col+'_stdfrommean'] >= 1, col+'_stdfrommeanbin'] = 1\n",
    "        dundee.drop([col+'_stdfrommean', col+'_std', col+'_mean'], axis=1, inplace=True)\n",
    "    return dundee\n",
    "\n",
    "#only BNC freq\n",
    "def get_bin_stdfrommean_freq(dundee):\n",
    "    col='BNCfreq'\n",
    "    dundee[col+'_mean'] = dundee[col].mean()\n",
    "    dundee[col+'_std'] = dundee[col].std()\n",
    "    dundee[col+'_stdfrommean'] = (dundee[col] - dundee[col+'_mean']) / dundee[col+'_std']\n",
    "    dundee[col+'_stdfrommeanbin'] = 0\n",
    "    dundee.loc[dundee[col+'_stdfrommean'] <= -1, col+'_stdfrommeanbin'] = 1 #smaller than - pay attention to infrequent words\n",
    "    dundee.drop([col+'_stdfrommean', col+'_std', col+'_mean'], axis=1, inplace=True)\n",
    "    return dundee\n",
    "\n",
    "dundee = get_bin_stdfrommean(dundee)\n",
    "zurich = get_bin_stdfrommean(zurich)\n",
    "dundee = get_bin_stdfrommean_freq(dundee)\n",
    "zurich = get_bin_stdfrommean_freq(zurich)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"#defining bins - taking min and max values from dundee and zurich as endpoints\n",
    "#making bins based on values from both geco and dundee\n",
    "bins= 7\n",
    "freqbins = pd.cut(pd.concat([zurich.BNCfreq, dundee.BNCfreq]), bins, retbins=True)\n",
    "dundee['Freqgroup'] = pd.cut(dundee.BNCfreq, bins = freqbins[1], labels = [i for i in range(bins)])\n",
    "zurich['Freqgroup'] = pd.cut(zurich.BNCfreq, bins = freqbins[1], labels = [i for i in range(bins)])\n",
    "\n",
    "wlenbins = pd.cut(pd.concat([zurich.Wordlen, dundee.WordLen]), bins, retbins=True)\n",
    "zurich['Wlengroup'] = pd.cut(zurich.Wordlen, bins = wlenbins[1], labels=[i for i in range(bins)])\n",
    "dundee['Wlengroup'] = pd.cut(dundee.WordLen, bins = wlenbins[1], labels = [i for i in range(bins)])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 564,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Wlengroup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-564-2824ab15937e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#taking the mean and std for each measure for each group\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdundeedatacols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mgroupbyitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdundee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wlengroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Freqgroup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mdundee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdundee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupbyitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_meangroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wlengroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Freqgroup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdundee\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdundee\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupbyitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_stdgroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wlengroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Freqgroup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[1;32m   5160\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   5161\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5162\u001b[0;31m                        **kwargs)\n\u001b[0m\u001b[1;32m   5163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5164\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                                                     \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, mutated, validate)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2935\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Wlengroup'"
     ]
    }
   ],
   "source": [
    "\"\"\"stdfrommean = 1 #positive label is >= stdfrommean\n",
    "#taking the mean and std for each measure for each group\n",
    "for colname in dundeedatacols:\n",
    "    groupbyitem = dundee.groupby(['Wlengroup', 'Freqgroup'])[colname]\n",
    "    dundee = dundee.join(groupbyitem.mean(), rsuffix='_meangroup', on=['Wlengroup', 'Freqgroup'])\n",
    "    dundee = dundee.join(groupbyitem.std(), rsuffix='_stdgroup', on=['Wlengroup', 'Freqgroup'])\n",
    "    dundee[colname+'_stdgroup'].fillna(1, inplace=True) #for the very few groups with only 1 word. They have no std.\n",
    "    dundee[colname+'_stdfrommean'] = (dundee[colname] - dundee[colname+'_meangroup']) / dundee[colname+'_stdgroup']\n",
    "    dundee.drop(colname+'_meangroup', axis=1, inplace=True)\n",
    "    dundee.drop(colname+'_stdgroup', axis=1, inplace=True)\n",
    "    #binarize _stdfrommean-cols\n",
    "    dundee.loc[dundee[colname+'_stdfrommean'] >= stdfrommean, colname+'_stdfrommean' ] = 1\n",
    "    dundee.loc[dundee[colname+'_stdfrommean'] < stdfrommean, colname+'_stdfrommean'] = 0\n",
    "    dundee[colname+'_stdfrommean'].replace({0: '-', 1: '+'}, inplace=True)\n",
    "    \n",
    "for colname in joint_datacols:\n",
    "    groupbyitem = zurich.groupby(['Wlengroup', 'Freqgroup'])[colname]\n",
    "    zurich = zurich.join(groupbyitem.mean(), rsuffix='_meangroup', on=['Wlengroup', 'Freqgroup'])\n",
    "    zurich = zurich.join(groupbyitem.std(), rsuffix='_stdgroup', on=['Wlengroup', 'Freqgroup'])\n",
    "    zurich[colname+'_stdgroup'].fillna(1, inplace=True) #for the very few groups with only 1 word. They have no std.\n",
    "    zurich[colname+'_stdfrommean'] = (zurich[colname] - zurich[colname+'_meangroup']) / zurich[colname+'_stdgroup']\n",
    "    zurich.drop(colname+'_meangroup', axis=1, inplace=True)\n",
    "    zurich.drop(colname+'_stdgroup', axis=1, inplace=True)\n",
    "    #binarize _stdfrommean-cols\n",
    "    zurich.loc[zurich[colname+'_stdfrommean'] >= stdfrommean, colname+'_stdfrommean' ] = 1\n",
    "    zurich.loc[zurich[colname+'_stdfrommean'] < stdfrommean, colname+'_stdfrommean'] = 0\n",
    "    zurich[colname+'_stdfrommean'].fillna(0, inplace=True) #First pass duration missing values - resulting in nans\n",
    "    zurich[colname+'_stdfrommean'].replace({0: '-', 1: '+'}, inplace=True)\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Use continous values min -max normalize\n",
    "#zurich.Mean_fix_dur.fillna(0, inplace=True) #could probably be commented out\n",
    "\n",
    "zurich[joint_datacols] = (zurich[joint_datacols]-zurich[joint_datacols].min())/(zurich[joint_datacols].max()-zurich[joint_datacols].min())\n",
    "dundee[joint_datacols] = (dundee[joint_datacols]-dundee[joint_datacols].min())/(dundee[joint_datacols].max()-dundee[joint_datacols].min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sent-id</th>\n",
       "      <th>word-id</th>\n",
       "      <th>Word</th>\n",
       "      <th>nFix</th>\n",
       "      <th>First_fix_dur</th>\n",
       "      <th>Tot_fix_dur</th>\n",
       "      <th>First_pass_dur</th>\n",
       "      <th>SFD</th>\n",
       "      <th>Mean_fix_dur</th>\n",
       "      <th>WORDstrip</th>\n",
       "      <th>WordLen</th>\n",
       "      <th>BNCfreq</th>\n",
       "      <th>BNCfreqinv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>0</td>\n",
       "      <td>Presents</td>\n",
       "      <td>0.156863</td>\n",
       "      <td>0.238623</td>\n",
       "      <td>0.124402</td>\n",
       "      <td>0.123658</td>\n",
       "      <td>42.416667</td>\n",
       "      <td>0.264039</td>\n",
       "      <td>Presents</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.499857</td>\n",
       "      <td>0.500143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>1</td>\n",
       "      <td>a</td>\n",
       "      <td>0.053922</td>\n",
       "      <td>0.111554</td>\n",
       "      <td>0.029020</td>\n",
       "      <td>0.048249</td>\n",
       "      <td>50.833333</td>\n",
       "      <td>0.116627</td>\n",
       "      <td>a</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.932427</td>\n",
       "      <td>0.067573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>2</td>\n",
       "      <td>good</td>\n",
       "      <td>0.151961</td>\n",
       "      <td>0.171838</td>\n",
       "      <td>0.098537</td>\n",
       "      <td>0.087935</td>\n",
       "      <td>19.750000</td>\n",
       "      <td>0.189884</td>\n",
       "      <td>good</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.722573</td>\n",
       "      <td>0.277427</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>3</td>\n",
       "      <td>case</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.186761</td>\n",
       "      <td>0.078655</td>\n",
       "      <td>0.115350</td>\n",
       "      <td>38.166667</td>\n",
       "      <td>0.208777</td>\n",
       "      <td>case</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.682662</td>\n",
       "      <td>0.317338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>4</td>\n",
       "      <td>while</td>\n",
       "      <td>0.147059</td>\n",
       "      <td>0.215278</td>\n",
       "      <td>0.096687</td>\n",
       "      <td>0.120782</td>\n",
       "      <td>17.166667</td>\n",
       "      <td>0.206363</td>\n",
       "      <td>while</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.699861</td>\n",
       "      <td>0.300139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>5</td>\n",
       "      <td>failing</td>\n",
       "      <td>0.181373</td>\n",
       "      <td>0.184840</td>\n",
       "      <td>0.126251</td>\n",
       "      <td>0.128195</td>\n",
       "      <td>21.916667</td>\n",
       "      <td>0.196909</td>\n",
       "      <td>failing</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.494119</td>\n",
       "      <td>0.505881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>6</td>\n",
       "      <td>to</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.088948</td>\n",
       "      <td>0.022710</td>\n",
       "      <td>0.038471</td>\n",
       "      <td>33.916667</td>\n",
       "      <td>0.091755</td>\n",
       "      <td>to</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.943570</td>\n",
       "      <td>0.056430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>7</td>\n",
       "      <td>provide</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.256206</td>\n",
       "      <td>0.112625</td>\n",
       "      <td>0.124233</td>\n",
       "      <td>22.750000</td>\n",
       "      <td>0.222849</td>\n",
       "      <td>provide</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.640313</td>\n",
       "      <td>0.359687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>8</td>\n",
       "      <td>a</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.055851</td>\n",
       "      <td>0.017733</td>\n",
       "      <td>0.024156</td>\n",
       "      <td>24.666667</td>\n",
       "      <td>0.061269</td>\n",
       "      <td>a</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.932427</td>\n",
       "      <td>0.067573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>9</td>\n",
       "      <td>reason</td>\n",
       "      <td>0.171569</td>\n",
       "      <td>0.221188</td>\n",
       "      <td>0.111075</td>\n",
       "      <td>0.151074</td>\n",
       "      <td>11.500000</td>\n",
       "      <td>0.208575</td>\n",
       "      <td>reason</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.628026</td>\n",
       "      <td>0.371974</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>10</td>\n",
       "      <td>for</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.062057</td>\n",
       "      <td>0.013463</td>\n",
       "      <td>0.031633</td>\n",
       "      <td>22.416667</td>\n",
       "      <td>0.056442</td>\n",
       "      <td>for</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.126812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>11</td>\n",
       "      <td>us</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.020981</td>\n",
       "      <td>0.003862</td>\n",
       "      <td>0.009075</td>\n",
       "      <td>11.833333</td>\n",
       "      <td>0.020981</td>\n",
       "      <td>us</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.720483</td>\n",
       "      <td>0.279517</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>12</td>\n",
       "      <td>to</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.125296</td>\n",
       "      <td>0.032936</td>\n",
       "      <td>0.060263</td>\n",
       "      <td>49.583333</td>\n",
       "      <td>0.122045</td>\n",
       "      <td>to</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.943570</td>\n",
       "      <td>0.056430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>13</td>\n",
       "      <td>care</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.188830</td>\n",
       "      <td>0.072128</td>\n",
       "      <td>0.090043</td>\n",
       "      <td>66.250000</td>\n",
       "      <td>0.207159</td>\n",
       "      <td>care</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.647318</td>\n",
       "      <td>0.352682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>14</td>\n",
       "      <td>beyond</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.165041</td>\n",
       "      <td>0.077023</td>\n",
       "      <td>0.085634</td>\n",
       "      <td>41.416667</td>\n",
       "      <td>0.179016</td>\n",
       "      <td>beyond</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.599714</td>\n",
       "      <td>0.400286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>15</td>\n",
       "      <td>the</td>\n",
       "      <td>0.019608</td>\n",
       "      <td>0.041962</td>\n",
       "      <td>0.010226</td>\n",
       "      <td>0.018149</td>\n",
       "      <td>16.166667</td>\n",
       "      <td>0.042110</td>\n",
       "      <td>the</td>\n",
       "      <td>0.12</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>16</td>\n",
       "      <td>very</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.175975</td>\n",
       "      <td>0.063751</td>\n",
       "      <td>0.092855</td>\n",
       "      <td>54.083333</td>\n",
       "      <td>0.185468</td>\n",
       "      <td>very</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.749493</td>\n",
       "      <td>0.250507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>17</td>\n",
       "      <td>basic</td>\n",
       "      <td>0.122549</td>\n",
       "      <td>0.193558</td>\n",
       "      <td>0.083578</td>\n",
       "      <td>0.099374</td>\n",
       "      <td>67.083333</td>\n",
       "      <td>0.212669</td>\n",
       "      <td>basic</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.596369</td>\n",
       "      <td>0.403631</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>18</td>\n",
       "      <td>dictums</td>\n",
       "      <td>0.142157</td>\n",
       "      <td>0.199025</td>\n",
       "      <td>0.098836</td>\n",
       "      <td>0.138740</td>\n",
       "      <td>20.833333</td>\n",
       "      <td>0.212579</td>\n",
       "      <td>dictums</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.102918</td>\n",
       "      <td>0.897082</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>19</td>\n",
       "      <td>of</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>0.016400</td>\n",
       "      <td>0.038535</td>\n",
       "      <td>50.250000</td>\n",
       "      <td>0.089096</td>\n",
       "      <td>of</td>\n",
       "      <td>0.08</td>\n",
       "      <td>0.952451</td>\n",
       "      <td>0.047549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>20</td>\n",
       "      <td>human</td>\n",
       "      <td>0.093137</td>\n",
       "      <td>0.211141</td>\n",
       "      <td>0.069272</td>\n",
       "      <td>0.100013</td>\n",
       "      <td>55.916667</td>\n",
       "      <td>0.217839</td>\n",
       "      <td>human</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.631948</td>\n",
       "      <td>0.368052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0_SR</td>\n",
       "      <td>21</td>\n",
       "      <td>decency.</td>\n",
       "      <td>0.073529</td>\n",
       "      <td>0.203014</td>\n",
       "      <td>0.051839</td>\n",
       "      <td>0.092728</td>\n",
       "      <td>60.083333</td>\n",
       "      <td>0.188362</td>\n",
       "      <td>decency</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.372740</td>\n",
       "      <td>0.627260</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>0</td>\n",
       "      <td>Beautifully</td>\n",
       "      <td>0.127451</td>\n",
       "      <td>0.175975</td>\n",
       "      <td>0.080151</td>\n",
       "      <td>0.111132</td>\n",
       "      <td>27.750000</td>\n",
       "      <td>0.194358</td>\n",
       "      <td>Beautifully</td>\n",
       "      <td>0.44</td>\n",
       "      <td>0.455636</td>\n",
       "      <td>0.544364</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>1</td>\n",
       "      <td>crafted,</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.214982</td>\n",
       "      <td>0.114883</td>\n",
       "      <td>0.141488</td>\n",
       "      <td>36.666667</td>\n",
       "      <td>0.225639</td>\n",
       "      <td>crafted</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.316000</td>\n",
       "      <td>0.684000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>2</td>\n",
       "      <td>engaging</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.228132</td>\n",
       "      <td>0.134628</td>\n",
       "      <td>0.129793</td>\n",
       "      <td>6.166667</td>\n",
       "      <td>0.250005</td>\n",
       "      <td>engaging</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.403263</td>\n",
       "      <td>0.596737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>3</td>\n",
       "      <td>filmmaking</td>\n",
       "      <td>0.137255</td>\n",
       "      <td>0.212175</td>\n",
       "      <td>0.093424</td>\n",
       "      <td>0.137653</td>\n",
       "      <td>40.166667</td>\n",
       "      <td>0.195635</td>\n",
       "      <td>filmmaking</td>\n",
       "      <td>0.40</td>\n",
       "      <td>0.257407</td>\n",
       "      <td>0.742593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>4</td>\n",
       "      <td>that</td>\n",
       "      <td>0.044118</td>\n",
       "      <td>0.107270</td>\n",
       "      <td>0.027306</td>\n",
       "      <td>0.051572</td>\n",
       "      <td>39.916667</td>\n",
       "      <td>0.101876</td>\n",
       "      <td>that</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.890434</td>\n",
       "      <td>0.109566</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>5</td>\n",
       "      <td>should</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.221631</td>\n",
       "      <td>0.060406</td>\n",
       "      <td>0.111708</td>\n",
       "      <td>78.750000</td>\n",
       "      <td>0.211251</td>\n",
       "      <td>should</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.743019</td>\n",
       "      <td>0.256981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>6</td>\n",
       "      <td>attract</td>\n",
       "      <td>0.078431</td>\n",
       "      <td>0.171247</td>\n",
       "      <td>0.049608</td>\n",
       "      <td>0.095987</td>\n",
       "      <td>66.666667</td>\n",
       "      <td>0.167984</td>\n",
       "      <td>attract</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.501460</td>\n",
       "      <td>0.498540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>7</td>\n",
       "      <td>upscale</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.225621</td>\n",
       "      <td>0.069109</td>\n",
       "      <td>0.121549</td>\n",
       "      <td>82.833333</td>\n",
       "      <td>0.228625</td>\n",
       "      <td>upscale</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.114576</td>\n",
       "      <td>0.885424</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>8</td>\n",
       "      <td>audiences</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.226655</td>\n",
       "      <td>0.093070</td>\n",
       "      <td>0.149859</td>\n",
       "      <td>37.166667</td>\n",
       "      <td>0.245124</td>\n",
       "      <td>audiences</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.442992</td>\n",
       "      <td>0.557008</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>9</td>\n",
       "      <td>hungry</td>\n",
       "      <td>0.112745</td>\n",
       "      <td>0.259013</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.117459</td>\n",
       "      <td>88.000000</td>\n",
       "      <td>0.247242</td>\n",
       "      <td>hungry</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.481512</td>\n",
       "      <td>0.518488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>10</td>\n",
       "      <td>for</td>\n",
       "      <td>0.039216</td>\n",
       "      <td>0.078753</td>\n",
       "      <td>0.024206</td>\n",
       "      <td>0.034062</td>\n",
       "      <td>18.166667</td>\n",
       "      <td>0.081856</td>\n",
       "      <td>for</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.873188</td>\n",
       "      <td>0.126812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>11</td>\n",
       "      <td>quality</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.222074</td>\n",
       "      <td>0.061494</td>\n",
       "      <td>0.116564</td>\n",
       "      <td>68.333333</td>\n",
       "      <td>0.214711</td>\n",
       "      <td>quality</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.620289</td>\n",
       "      <td>0.379711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>12</td>\n",
       "      <td>and</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.082151</td>\n",
       "      <td>0.018059</td>\n",
       "      <td>0.035532</td>\n",
       "      <td>34.583333</td>\n",
       "      <td>0.079713</td>\n",
       "      <td>and</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.946559</td>\n",
       "      <td>0.053441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>13</td>\n",
       "      <td>a</td>\n",
       "      <td>0.029412</td>\n",
       "      <td>0.076093</td>\n",
       "      <td>0.016944</td>\n",
       "      <td>0.032912</td>\n",
       "      <td>35.583333</td>\n",
       "      <td>0.077571</td>\n",
       "      <td>a</td>\n",
       "      <td>0.04</td>\n",
       "      <td>0.932427</td>\n",
       "      <td>0.067573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>14</td>\n",
       "      <td>nostalgic,</td>\n",
       "      <td>0.151961</td>\n",
       "      <td>0.216903</td>\n",
       "      <td>0.104901</td>\n",
       "      <td>0.159190</td>\n",
       "      <td>29.583333</td>\n",
       "      <td>0.202152</td>\n",
       "      <td>nostalgic</td>\n",
       "      <td>0.36</td>\n",
       "      <td>0.359404</td>\n",
       "      <td>0.640596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>15</td>\n",
       "      <td>twisty</td>\n",
       "      <td>0.083333</td>\n",
       "      <td>0.208629</td>\n",
       "      <td>0.056163</td>\n",
       "      <td>0.098223</td>\n",
       "      <td>69.833333</td>\n",
       "      <td>0.197178</td>\n",
       "      <td>twisty</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.210757</td>\n",
       "      <td>0.789243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>16</td>\n",
       "      <td>yarn</td>\n",
       "      <td>0.024510</td>\n",
       "      <td>0.101655</td>\n",
       "      <td>0.018712</td>\n",
       "      <td>0.043967</td>\n",
       "      <td>57.333333</td>\n",
       "      <td>0.101655</td>\n",
       "      <td>yarn</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.433333</td>\n",
       "      <td>0.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1_SR</td>\n",
       "      <td>17</td>\n",
       "      <td>that</td>\n",
       "      <td>0.098039</td>\n",
       "      <td>0.142878</td>\n",
       "      <td>0.061249</td>\n",
       "      <td>0.095795</td>\n",
       "      <td>43.833333</td>\n",
       "      <td>0.157998</td>\n",
       "      <td>that</td>\n",
       "      <td>0.16</td>\n",
       "      <td>0.890434</td>\n",
       "      <td>0.109566</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sent-id  word-id         Word      nFix  First_fix_dur  Tot_fix_dur  \\\n",
       "0     0_SR        0     Presents  0.156863       0.238623     0.124402   \n",
       "1     0_SR        1            a  0.053922       0.111554     0.029020   \n",
       "2     0_SR        2         good  0.151961       0.171838     0.098537   \n",
       "3     0_SR        3         case  0.102941       0.186761     0.078655   \n",
       "4     0_SR        4        while  0.147059       0.215278     0.096687   \n",
       "5     0_SR        5      failing  0.181373       0.184840     0.126251   \n",
       "6     0_SR        6           to  0.039216       0.088948     0.022710   \n",
       "7     0_SR        7      provide  0.166667       0.256206     0.112625   \n",
       "8     0_SR        8            a  0.029412       0.055851     0.017733   \n",
       "9     0_SR        9       reason  0.171569       0.221188     0.111075   \n",
       "10    0_SR       10          for  0.024510       0.062057     0.013463   \n",
       "11    0_SR       11           us  0.004902       0.020981     0.003862   \n",
       "12    0_SR       12           to  0.044118       0.125296     0.032936   \n",
       "13    0_SR       13         care  0.107843       0.188830     0.072128   \n",
       "14    0_SR       14       beyond  0.127451       0.165041     0.077023   \n",
       "15    0_SR       15          the  0.019608       0.041962     0.010226   \n",
       "16    0_SR       16         very  0.098039       0.175975     0.063751   \n",
       "17    0_SR       17        basic  0.122549       0.193558     0.083578   \n",
       "18    0_SR       18      dictums  0.142157       0.199025     0.098836   \n",
       "19    0_SR       19           of  0.024510       0.089096     0.016400   \n",
       "20    0_SR       20        human  0.093137       0.211141     0.069272   \n",
       "21    0_SR       21     decency.  0.073529       0.203014     0.051839   \n",
       "22    1_SR        0  Beautifully  0.127451       0.175975     0.080151   \n",
       "23    1_SR        1     crafted,  0.166667       0.214982     0.114883   \n",
       "24    1_SR        2     engaging  0.161765       0.228132     0.134628   \n",
       "25    1_SR        3   filmmaking  0.137255       0.212175     0.093424   \n",
       "26    1_SR        4         that  0.044118       0.107270     0.027306   \n",
       "27    1_SR        5       should  0.088235       0.221631     0.060406   \n",
       "28    1_SR        6      attract  0.078431       0.171247     0.049608   \n",
       "29    1_SR        7      upscale  0.088235       0.225621     0.069109   \n",
       "30    1_SR        8    audiences  0.117647       0.226655     0.093070   \n",
       "31    1_SR        9       hungry  0.112745       0.259013     0.083333   \n",
       "32    1_SR       10          for  0.039216       0.078753     0.024206   \n",
       "33    1_SR       11      quality  0.083333       0.222074     0.061494   \n",
       "34    1_SR       12          and  0.029412       0.082151     0.018059   \n",
       "35    1_SR       13            a  0.029412       0.076093     0.016944   \n",
       "36    1_SR       14   nostalgic,  0.151961       0.216903     0.104901   \n",
       "37    1_SR       15       twisty  0.083333       0.208629     0.056163   \n",
       "38    1_SR       16         yarn  0.024510       0.101655     0.018712   \n",
       "39    1_SR       17         that  0.098039       0.142878     0.061249   \n",
       "\n",
       "    First_pass_dur        SFD  Mean_fix_dur    WORDstrip  WordLen   BNCfreq  \\\n",
       "0         0.123658  42.416667      0.264039     Presents     0.32  0.499857   \n",
       "1         0.048249  50.833333      0.116627            a     0.04  0.932427   \n",
       "2         0.087935  19.750000      0.189884         good     0.16  0.722573   \n",
       "3         0.115350  38.166667      0.208777         case     0.16  0.682662   \n",
       "4         0.120782  17.166667      0.206363        while     0.20  0.699861   \n",
       "5         0.128195  21.916667      0.196909      failing     0.28  0.494119   \n",
       "6         0.038471  33.916667      0.091755           to     0.08  0.943570   \n",
       "7         0.124233  22.750000      0.222849      provide     0.28  0.640313   \n",
       "8         0.024156  24.666667      0.061269            a     0.04  0.932427   \n",
       "9         0.151074  11.500000      0.208575       reason     0.24  0.628026   \n",
       "10        0.031633  22.416667      0.056442          for     0.12  0.873188   \n",
       "11        0.009075  11.833333      0.020981           us     0.08  0.720483   \n",
       "12        0.060263  49.583333      0.122045           to     0.08  0.943570   \n",
       "13        0.090043  66.250000      0.207159         care     0.16  0.647318   \n",
       "14        0.085634  41.416667      0.179016       beyond     0.24  0.599714   \n",
       "15        0.018149  16.166667      0.042110          the     0.12  1.000000   \n",
       "16        0.092855  54.083333      0.185468         very     0.16  0.749493   \n",
       "17        0.099374  67.083333      0.212669        basic     0.20  0.596369   \n",
       "18        0.138740  20.833333      0.212579      dictums     0.28  0.102918   \n",
       "19        0.038535  50.250000      0.089096           of     0.08  0.952451   \n",
       "20        0.100013  55.916667      0.217839        human     0.20  0.631948   \n",
       "21        0.092728  60.083333      0.188362      decency     0.28  0.372740   \n",
       "22        0.111132  27.750000      0.194358  Beautifully     0.44  0.455636   \n",
       "23        0.141488  36.666667      0.225639      crafted     0.28  0.316000   \n",
       "24        0.129793   6.166667      0.250005     engaging     0.32  0.403263   \n",
       "25        0.137653  40.166667      0.195635   filmmaking     0.40  0.257407   \n",
       "26        0.051572  39.916667      0.101876         that     0.16  0.890434   \n",
       "27        0.111708  78.750000      0.211251       should     0.24  0.743019   \n",
       "28        0.095987  66.666667      0.167984      attract     0.28  0.501460   \n",
       "29        0.121549  82.833333      0.228625      upscale     0.28  0.114576   \n",
       "30        0.149859  37.166667      0.245124    audiences     0.36  0.442992   \n",
       "31        0.117459  88.000000      0.247242       hungry     0.24  0.481512   \n",
       "32        0.034062  18.166667      0.081856          for     0.12  0.873188   \n",
       "33        0.116564  68.333333      0.214711      quality     0.28  0.620289   \n",
       "34        0.035532  34.583333      0.079713          and     0.12  0.946559   \n",
       "35        0.032912  35.583333      0.077571            a     0.04  0.932427   \n",
       "36        0.159190  29.583333      0.202152    nostalgic     0.36  0.359404   \n",
       "37        0.098223  69.833333      0.197178       twisty     0.24  0.210757   \n",
       "38        0.043967  57.333333      0.101655         yarn     0.16  0.433333   \n",
       "39        0.095795  43.833333      0.157998         that     0.16  0.890434   \n",
       "\n",
       "    BNCfreqinv  \n",
       "0     0.500143  \n",
       "1     0.067573  \n",
       "2     0.277427  \n",
       "3     0.317338  \n",
       "4     0.300139  \n",
       "5     0.505881  \n",
       "6     0.056430  \n",
       "7     0.359687  \n",
       "8     0.067573  \n",
       "9     0.371974  \n",
       "10    0.126812  \n",
       "11    0.279517  \n",
       "12    0.056430  \n",
       "13    0.352682  \n",
       "14    0.400286  \n",
       "15    0.000000  \n",
       "16    0.250507  \n",
       "17    0.403631  \n",
       "18    0.897082  \n",
       "19    0.047549  \n",
       "20    0.368052  \n",
       "21    0.627260  \n",
       "22    0.544364  \n",
       "23    0.684000  \n",
       "24    0.596737  \n",
       "25    0.742593  \n",
       "26    0.109566  \n",
       "27    0.256981  \n",
       "28    0.498540  \n",
       "29    0.885424  \n",
       "30    0.557008  \n",
       "31    0.518488  \n",
       "32    0.126812  \n",
       "33    0.379711  \n",
       "34    0.053441  \n",
       "35    0.067573  \n",
       "36    0.640596  \n",
       "37    0.789243  \n",
       "38    0.566667  \n",
       "39    0.109566  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zurich."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4651"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(zurich.groupby('WORDstrip'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#insert blank line after each sentence - for GECO we don't have sentence boundaries\n",
    "zurichblank = pd.Series([np.nan]*len(zurich.columns), index=zurich.columns)\n",
    "zurich = zurich.groupby(['sent-id'], as_index=False).apply(lambda x: x.append(zurichblank, ignore_index=True)).reset_index(drop=True)\n",
    "\n",
    "dundeeblank = pd.Series([np.nan]*len(dundee.columns), index=dundee.columns)\n",
    "dundee = dundee.groupby(['Itemno', 'SentenceID'], as_index=False).apply(lambda x: x.append(dundeeblank, ignore_index=True)).reset_index(drop=True)\n",
    "\n",
    "#concatenate dundee and zurich\n",
    "\n",
    "both = pd.concat([zurich[word_and_joint], dundee[word_and_joint]]).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for colname in joint_datacols:\n",
    "#    dundee[['Word', colname+'_stdfrommean']].to_csv('exp_data/dundee'+colname+'.csv', sep='\\t', encoding='utf-8', index=None, header=None)\n",
    "     \n",
    "#for colname in joint_datacols:\n",
    "#    zurich[['Word', colname]].to_csv('exp_data/zurich_nogroup_SR'+colname+'.csv', sep='\\t', encoding='utf-8', index=None, header=None)\n",
    "    \n",
    "for colname in joint_datacols:\n",
    "    both[['Word', colname]].to_csv('exp_data/both_cont_'+colname+'.csv', sep='\\t', encoding='utf-8', index=None, header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 574,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_input_files(file_paths, max_sentence_length=-1):\n",
    "    \"\"\"\n",
    "    Reads input files in whitespace-separated format.\n",
    "    Will split file_paths on comma, reading from multiple files.\n",
    "    \"\"\"\n",
    "    # JB: I've changed this so a sentence-level label can be read at the\n",
    "    # beginning of the sentence (single label on extra line preceding sentence)\n",
    "    # output is a list of (sentence, label) tuples, where sentence is a list of\n",
    "    # (token, token_label) tuples\n",
    "    sentences = []\n",
    "    line_length = None\n",
    "    sent_label = \"\"\n",
    "    for file_path in file_paths.strip().split(\",\"):\n",
    "        with open(file_path, \"r\") as f:\n",
    "            ttsentence = []\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                if len(line) == 1:\n",
    "                    sent_label = line\n",
    "                elif len(line) > 1:\n",
    "                    line_parts = line.split()\n",
    "                    assert(len(line_parts) >= 2), line\n",
    "                    assert(len(line_parts) == line_length or line_length == None)\n",
    "                    line_length = len(line_parts)\n",
    "                    sentence.append(line_parts)\n",
    "                elif len(line) == 0 and len(sentence) > 0:\n",
    "                    if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n",
    "                        sentences.append((sentence, sent_label))\n",
    "                    sentence = []\n",
    "                    sent_label = \"\"\n",
    "            if len(sentence) > 0:\n",
    "                if max_sentence_length <= 0 or len(sentence) <= max_sentence_length:\n",
    "                    sentences.append((sentence, sent_label))\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = read_input_files('exp_data/both_cont_Mean_fix_dur.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semeval2014"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = codecs.open('Data/SemEval2014-Task9-subtaskAB-test-to-download/SemEval2014-task9-test-B-gold.txt', 'r', 'utf-8').read().split('\\n')\n",
    "sents = [li.split('\\t') for li in f]\n",
    "posfile=codecs.open(\"Data/SemEval2014-Task9-subtaskAB-test-to-download/Semeval2014.test.pos\", mode='w', encoding='utf-8')\n",
    "negfile =codecs.open(\"Data/SemEval2014-Task9-subtaskAB-test-to-download/Semeval14.test.neg\", mode='w', encoding='utf-8')\n",
    "\n",
    "for sent in sents:\n",
    "    if len(sent)==4 and sent[3] != 'Not Available':\n",
    "        if sent[2] == 'neutral':\n",
    "            print('-', file=posfile)\n",
    "            print('-', file=negfile)\n",
    "            words = sent[3]\n",
    "            words = words.split()\n",
    "            [print(word+'\\t_', file=negfile) for word in words]\n",
    "            [print(word+'\\t_', file=posfile) for word in words]\n",
    "            print('\\n', file=posfile)\n",
    "            print('\\n', file=negfile)\n",
    "            \n",
    "        elif sent[2] == 'positive':\n",
    "            print('+', file=posfile)\n",
    "            words = sent[3]\n",
    "            words = words.split()\n",
    "            [print(word+'\\t_', file=posfile) for word in words]\n",
    "            print('\\n', file=posfile)\n",
    "        elif sent[2] == 'negative':\n",
    "            print('+', file=negfile)\n",
    "            [print(word+'\\t_', file=negfile) for word in words]\n",
    "            print('\\n', file=negfile)\n",
    "posfile.close()\n",
    "negfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = codecs.open('Data/SemEval2014-Task9-subtaskAB-test-to-download/SemEval2014-task9-dev-B-gold.txt', 'r', 'utf-8').read().split('\\n')\n",
    "sents = [li.split('\\t') for li in f]\n",
    "posfile=codecs.open(\"Data/SemEval2014-Task9-subtaskAB-test-to-download/Semeval2014.dev.pos\", mode='w', encoding='utf-8')\n",
    "negfile =codecs.open(\"Data/SemEval2014-Task9-subtaskAB-test-to-download/Semeval14.dev.neg\", mode='w', encoding='utf-8')\n",
    "\n",
    "for sent in sents:\n",
    "    if len(sent)==4 and sent[3] != 'Not Available':\n",
    "        if sent[2] == 'neutral':\n",
    "            print('-', file=posfile)\n",
    "            print('-', file=negfile)\n",
    "            words = sent[3]\n",
    "            words = words.split()\n",
    "            [print(word+'\\t_', file=negfile) for word in words]\n",
    "            [print(word+'\\t_', file=posfile) for word in words]\n",
    "            print('\\n', file=posfile)\n",
    "            print('\\n', file=negfile)\n",
    "            \n",
    "        elif sent[2] == 'positive':\n",
    "            print('+', file=posfile)\n",
    "            words = sent[3]\n",
    "            words = words.split()\n",
    "            [print(word+'\\t_', file=posfile) for word in words]\n",
    "            print('\\n', file=posfile)\n",
    "        elif sent[2] == 'negative':\n",
    "            print('+', file=negfile)\n",
    "            [print(word+'\\t_', file=negfile) for word in words]\n",
    "            print('\\n', file=negfile)\n",
    "posfile.close()\n",
    "negfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Amazon reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As one task? Just test set? Is binary task - no neutral reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import random\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(pathtofiles):\n",
    "    random.seed(99)\n",
    "    labs = ['+', '-']\n",
    "    reviews = []\n",
    "    for i, fname in enumerate(['positive.review', 'negative.review']):\n",
    "        soup = BeautifulSoup(open(pathtofiles+fname), 'xml')\n",
    "        for review in soup.find_all('review'):\n",
    "            reviewsents = sent_tokenize(review.review_text.text.strip('\\n'))\n",
    "            for sent in reviewsents:\n",
    "                temp = []\n",
    "                temp.append(labs[i])\n",
    "\n",
    "                temp.append(sent)\n",
    "\n",
    "                reviews.append(temp)\n",
    "    random.shuffle(reviews)\n",
    "    \n",
    "    dev_size = 0.1\n",
    "    tr_size = 0.7\n",
    "    te_size = 0.2\n",
    "    \n",
    "    lenghtofreview = len(reviews)\n",
    "\n",
    "    tr_startidx=int(lenghtofreview*dev_size)\n",
    "    te_start_idx=np.int(lenghtofreview*(dev_size+tr_size))\n",
    "    \n",
    "    dv_reviews = reviews[:tr_startidx]\n",
    "    tr_reviews = reviews[tr_startidx:te_start_idx]\n",
    "    te_reviews = reviews[te_start_idx:]\n",
    "    \n",
    "    return dv_reviews, tr_reviews, te_reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "kitchen_dv, kitchen_tr, kitchen_te = parse_reviews('Data/Amazon_sorted_data/kitchen_&_housewares/')\n",
    "electronics_dv, electronics_tr, electronics_te = parse_reviews('Data/Amazon_sorted_data/electronics/')\n",
    "books_dv, books_tr, books_te = parse_reviews('Data/Amazon_sorted_data/books/')\n",
    "dvd_dv, dvd_tr, dvd_te = parse_reviews('Data/Amazon_sorted_data/dvd/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2340\n",
      "3480\n",
      "3813\n",
      "2661\n"
     ]
    }
   ],
   "source": [
    "print(len(kitchen_te))\n",
    "print(len(books_te))\n",
    "print(len(dvd_te))\n",
    "print(len(electronics_te))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [],
   "source": [
    "fnames = ['kitchen_dv', 'kitchen_tr', 'kitchen_te', 'electronics_dv', 'electronics_tr', 'electronics_te', \n",
    "          'books_dv', 'books_tr', 'books_te','dvd_dv', 'dvd_tr', 'dvd_te']\n",
    "\n",
    "\n",
    "for i, domain in enumerate([kitchen_dv, kitchen_tr, kitchen_te, electronics_dv, electronics_tr, electronics_te, \n",
    "          books_dv, books_tr, books_te, dvd_dv, dvd_tr, dvd_te]):\n",
    "    outfile = codecs.open('Data/Amazon_sorted_data/'+fnames[i]+'.txt', 'w', encoding='utf-8')\n",
    "    for sent in domain:\n",
    "        print(sent[0], file=outfile)\n",
    "        words = sent[1].split()\n",
    "        [print(word+'\\t_', file=outfile) for word in words]\n",
    "        print('\\n', file=outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Geco"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'Wlengroup'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-e378a86b20d8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcolname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mgecodatacols\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0mgeco\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeco\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0mgroupbyitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wlengroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Freqgroup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcolname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mgeco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupbyitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_meangroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wlengroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Freqgroup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0mgeco\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgeco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgroupbyitem\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrsuffix\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'_stdgroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Wlengroup'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Freqgroup'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/generic.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(self, by, axis, level, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[1;32m   5160\u001b[0m         return groupby(self, by=by, axis=axis, level=level, as_index=as_index,\n\u001b[1;32m   5161\u001b[0m                        \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroup_keys\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgroup_keys\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msqueeze\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5162\u001b[0;31m                        **kwargs)\n\u001b[0m\u001b[1;32m   5163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5164\u001b[0m     def asfreq(self, freq, method=None, how=None, normalize=False,\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36mgroupby\u001b[0;34m(obj, by, **kwds)\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'invalid type: %s'\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1847\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1848\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mklass\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mby\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, obj, keys, axis, level, grouper, exclusions, selection, as_index, sort, group_keys, squeeze, **kwargs)\u001b[0m\n\u001b[1;32m    514\u001b[0m                                                     \u001b[0mlevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m                                                     \u001b[0msort\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 516\u001b[0;31m                                                     mutated=self.mutated)\n\u001b[0m\u001b[1;32m    517\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    518\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/groupby.py\u001b[0m in \u001b[0;36m_get_grouper\u001b[0;34m(obj, key, axis, level, sort, mutated, validate)\u001b[0m\n\u001b[1;32m   2932\u001b[0m                 \u001b[0min_axis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlevel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2933\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2934\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2935\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgpr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGrouper\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mgpr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkey\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2936\u001b[0m             \u001b[0;31m# Add key to exclusions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'Wlengroup'"
     ]
    }
   ],
   "source": [
    "geco = pd.read_csv('Data/GECOmonoling.csv', encoding='utf-8', sep='\\t')\n",
    "\n",
    "gecodatacols = ['WORD_AVERAGE_FIX_PUPIL_SIZE',\n",
    "       'WORD_FIXATION_COUNT', 'WORD_RUN_COUNT',\n",
    "       'WORD_FIRST_RUN_FIXATION_COUNT',\n",
    "       'WORD_GAZE_DURATION', 'WORD_SECOND_RUN_START_TIME',\n",
    "       'WORD_SECOND_RUN_END_TIME', 'WORD_SECOND_RUN_FIXATION_COUNT',\n",
    "        'WORD_THIRD_RUN_FIXATION_COUNT', 'WORD_FIRST_FIXATION_DURATION',\n",
    "       'WORD_FIRST_FIXATION_TIME', 'WORD_FIRST_FIXATION_VISITED_WORD_COUNT',\n",
    "       'WORD_FIRST_FIX_PROGRESSIVE', 'WORD_SECOND_FIXATION_DURATION',\n",
    "       'WORD_SECOND_FIXATION_RUN', 'WORD_SECOND_FIXATION_TIME',\n",
    "       'WORD_THIRD_FIXATION_DURATION', 'WORD_THIRD_FIXATION_RUN',\n",
    "       'WORD_THIRD_FIXATION_TIME', 'WORD_LAST_FIXATION_DURATION',\n",
    "       'WORD_LAST_FIXATION_RUN', 'WORD_LAST_FIXATION_TIME', 'WORD_GO_PAST_TIME',\n",
    "       'WORD_SELECTIVE_GO_PAST_TIME', 'WORD_TOTAL_READING_TIME', 'WORD_SPILLOVER', 'WORD_SKIP',\n",
    "       'fix_prob']\n",
    "\n",
    "for colname in gecodatacols:\n",
    "    geco[colname] = geco[colname].astype(float)\n",
    "    groupbyitem = geco.groupby(['Wlengroup', 'Freqgroup'])[colname]\n",
    "    geco = geco.join(groupbyitem.mean(), rsuffix='_meangroup', on=['Wlengroup', 'Freqgroup'])\n",
    "    geco = geco.join(groupbyitem.std(), rsuffix='_stdgroup', on=['Wlengroup', 'Freqgroup'])\n",
    "    geco[colname+'_stdgroup'].fillna(1, inplace=True) #for the very few groups with only 1 word. They have no std.\n",
    "    geco[colname+'_stdfrommean'] = (geco[colname] - geco[colname+'_meangroup']) / geco[colname+'_stdgroup']\n",
    "    geco.drop(colname+'_meangroup', axis=1, inplace=True)\n",
    "    geco.drop(colname+'_stdgroup', axis=1, inplace=True)\n",
    "    #binarize _stdfrommean-cols\n",
    "    geco.loc[geco[colname+'_stdfrommean'] >= 0, colname+'_stdfrommean'] = 1\n",
    "    geco.loc[geco[colname+'_stdfrommean'] < 0, colname+'_stdfrommean'] = 0\n",
    "    geco[colname+'_stdfrommean'].replace({0: '-', 1: '+'}, inplace=True)\n",
    "    \n",
    "geco['WORDstrip'] = geco['WORD'].astype('str')\n",
    "geco.WORDstrip = geco.WORDstrip.str.replace('[^\\w\\s]','')\n",
    "geco['Wordlen'] = geco.WORDstrip.str.len()\n",
    "geco['BNCfreq'] = geco.WORDstrip.map(lambda x: unigrdict.get(x.lower()))\n",
    "geco.BNCfreq = geco.BNCfreq.fillna(geco.BNCfreq.min())\n",
    "geco.BNCfreq = geco.BNCfreq/1000000\n",
    "geco.BNCfreq = np.log(geco.BNCfreq)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# change c / i to + and - in fce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in glob.glob('Data/test/fce-error-detection/tsv/*original.tsv',encoding='utf-8' ):\n",
    "    df = pd.read_csv(fname, header=None, names=['word', 'label'], index_col=None, sep='\\s+', quoting=csv.QUOTE_NONE, skip_blank_lines=False)\n",
    "    df['label'].replace('c', '-', inplace=True)\n",
    "    df['label'].replace('i', '+', inplace=True)\n",
    "    df.to_csv(fname, header=None, index=None, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# make sentence-level labels for FCE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 425,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentlabel(foldername, ext):\n",
    "    #all files with ext in foldername are given sentence label + if there is one + in the tokenlevel labels. \n",
    "    #Otherwise -\n",
    "    for fname in glob.glob(foldername+'*'+'.'+ext):\n",
    "        f = open(fname, mode='r', encoding='utf-8').read().split('\\n\\t')\n",
    "        outfile = open(fname[:-4]+'sentlab'+'.'+ext, mode='w', encoding='utf-8')\n",
    "        content = [[l.split('\\t') for l in li.split('\\n')] for li in f]\n",
    "        for sent in content:\n",
    "            toklabs = [li[1] for li in sent if len(li) > 1]\n",
    "            if '+' in set(toklabs):\n",
    "                print(\"+\", file=outfile)     \n",
    "            else:\n",
    "                print(\"-\", file=outfile)\n",
    "            [print(li[0]+'\\t_', file=outfile) for li in sent if len(li) ==2]\n",
    "            print(\"\\n\", file=outfile, end='')\n",
    "        outfile.close()\n",
    "\n",
    "sentlabel('Data/fce-error-detection/tsv/', 'tsv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentiment semeval labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert 0 and P / N to + and - for tokenfiles\n",
    "for fname in glob.glob('Data/semeval15t10/semeval15t10.tokens*.tsv'):\n",
    "    df = pd.read_csv(fname, header=None, names=['word', 'label'], index_col=None, sep='\\s+', quoting=csv.QUOTE_NONE, skip_blank_lines=False)\n",
    "    df['label'].replace('O', '-', inplace=True)\n",
    "    df['label'].replace('P', '+', inplace=True)\n",
    "    df['label'].replace('N', '+', inplace=True)\n",
    "    df.to_csv(fname, header=None, index=None, encoding='utf-8', sep='\\t')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data/test2013twitter/semeval15t10.sentences.negative.test2013twitter.tsv\n",
      "Data/test2013twitter/semeval15t10.sentences.positive.test2013twitter.tsv\n"
     ]
    }
   ],
   "source": [
    "#get sentence level label from tokens and put in beginning of line\n",
    "#sent-annotated sentences are not aligned with token-level annotated, so we can't use sentlabel on token\n",
    "\n",
    "for fname in glob.glob('Data/test2013twitter/semeval15t10.sentences.*.test2013twitter.tsv'):\n",
    "    print(fname)\n",
    "    outfile = open(fname[:-4]+'sentlab'+'.tsv', mode='w', encoding='utf-8')\n",
    "    sentfile = open(fname, mode='r', encoding='utf-8').read().split('\\n\\n')\n",
    "    sentcontent = [[l.split('\\t') for l in li.split('\\n')] for li in sentfile]\n",
    "    \n",
    "    for sent in sentcontent:\n",
    "        sentlabs = [li[1] for li in sent if len(li) > 1]\n",
    "        if 'O' not in set(sentlabs) and len(set(sentlabs)) > 0: #sent use O, P and N\n",
    "            print(\"+\", file=outfile)     \n",
    "        else:\n",
    "            print(\"-\", file=outfile)\n",
    "        [print(li[0]+'\\t'+'_', file=outfile) for li in sent if len(li) ==2] # blank token label\n",
    "        print(\"\\n\", file=outfile, end='')\n",
    "outfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Draft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 523,
   "metadata": {},
   "outputs": [],
   "source": [
    "resfile = 'mltagger/exp/fce.dundee.First_fix_dur/output.txt'\n",
    "task = 'fce'\n",
    "split='dev'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 565,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['cost_sum', '9950.793182373047'],\n",
       " ['cost_avg', '3.855402240361506'],\n",
       " ['sent_count', '2581.0'],\n",
       " ['sent_predicted', '1629.0'],\n",
       " ['sent_correct', '1069.0'],\n",
       " ['sent_total', '1571.0'],\n",
       " ['sent_p', '0.6562308164518109'],\n",
       " ['sent_r', '0.6804583068109484'],\n",
       " ['sent_f', '0.668125'],\n",
       " ['sent_f05', '0.6609373067886731'],\n",
       " ['sent_correct_binary', '1519.0'],\n",
       " ['sent_accuracy_binary', '0.5885315769081751'],\n",
       " ['tok_0_map', '0.32840218880676103'],\n",
       " ['tok_0_p', '0.1502044863056141'],\n",
       " ['tok_0_r', '0.4680440239428461'],\n",
       " ['tok_0_f', '0.22742412159309472'],\n",
       " ['tok_0_f05', '0.17381078716783066'],\n",
       " ['time', '28.54233431816101'],\n",
       " ['best_epoch', '1']]"
      ]
     },
     "execution_count": 565,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'best_epoch': 1.0,\n",
       " 'cost_avg': 3.855402240361506,\n",
       " 'cost_sum': 9950.793182373047,\n",
       " 'sent_': 0.668125,\n",
       " 'sent_accuracy_binary': 0.5885315769081751,\n",
       " 'sent_correct': 1069.0,\n",
       " 'sent_correct_binary': 1519.0,\n",
       " 'sent_count': 2581.0,\n",
       " 'sent_f05': 0.6609373067886731,\n",
       " 'sent_p': 0.6562308164518109,\n",
       " 'sent_predicted': 1629.0,\n",
       " 'sent_r': 0.6804583068109484,\n",
       " 'sent_total': 1571.0,\n",
       " 'tim': 28.54233431816101,\n",
       " 'tok_0_': 0.22742412159309472,\n",
       " 'tok_0_f05': 0.17381078716783066,\n",
       " 'tok_0_map': 0.32840218880676103,\n",
       " 'tok_0_p': 0.1502044863056141,\n",
       " 'tok_0_r': 0.4680440239428461}"
      ]
     },
     "execution_count": 560,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# generate synthetic data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from  builtins import any as b_any\n",
    "\n",
    "for fname in ['tr', 'dv', 'te']:\n",
    "    outfile = open('Data/toy/'+fname+'.tsv', mode='w', encoding='utf-8')\n",
    "    for i in range(50):\n",
    "        wordsents = []\n",
    "        for j in range(random.randint(5,10)):\n",
    "            wordsents.append(''.join(random.choices(['a']*15+['A'], k=random.randint(2,5))))\n",
    "        if b_any('A' in x for x in wordsents):\n",
    "            print('+', file=outfile)\n",
    "        else:\n",
    "            print('-', file=outfile)\n",
    "        [print(word+'\\t'+'_',  file=outfile) for word in wordsents]\n",
    "        print('\\n', file=outfile, end='')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "for fname in ['gazetr']:\n",
    "    outfile = open('Data/toy/'+fname+'.tsv', mode='w', encoding='utf-8')\n",
    "    for i in range(50):\n",
    "        wordsents = []\n",
    "        for j in range(random.randint(5,10)):\n",
    "            wordsents.append(''.join(random.choices(['a']*15+['A'], k=random.randint(2,5))))\n",
    "        for word in wordsents:\n",
    "            if 'A' in word:\n",
    "                print(word+'\\t'+'+', file=outfile)\n",
    "            elif 'A' not in word:\n",
    "                print(word+'\\t'+'-', file=outfile)\n",
    "        print('\\n', file=outfile, end='')\n",
    "    outfile.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frdundee = pd.read_csv()"
   ]
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
